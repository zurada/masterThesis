\chapter{Wnioski w odniesieniu do stworzonych aplikacji}
W tym rozdziale autor tekstu prezentuje własne wnioski w odniesieniu do stworzonych aplikacji przedstawiających dwa przeciwstawne podejścia do architektury systemów informatycznych. Pierwszym podejściem jest użycie struktury monolitycznej,\index{aplikacja monolityczna} czyli takiej gdzie konstrukcja oprogramowania oparta jest na jednym, wspólnym i współdzielonym modelu danych. Przeciwstawnym jest wzorzec Event Sourcing\index{Event Sourcing} w połączeniu ze wzorcem CQRS\index{CQRS} i podejściem mikroserwisowym. Co jest warte uwagi, to to, iż każdy z obu sposobów na tworzenie architektury systemów jest poprawny i oba spotykane są na co dzień w pracy programisty. Jedynie kontekst ich użycia jest kluczowy. Przez kontekst rozumiany jest typ aplikacji, skomplikowaność domeny biznesowej, możliwości i wymagania projektowe. Dlatego autor pracy stara się przedstawić własne wnioski kiedy i w jakim kontekście dany wybór jest właściwy. W dalszej części rozdziału zostaje porównana większość cech jakości oprogramowania obu podejść. Ażeby jeszcze wyostrzyć kontrast różnic między oba podejściami, użyty został model anemiczny przy tworzeniu aplikacji monolitycznej.

\section{Wybór podejścia}
Na początkowym etapie tworzenia systemu informatycznego najważniejszym wyborem jest decyzja o podejściu architektonicznym. 
Decyzja pociąga za sobą bardzo dużo konsekwencji projektowych. Zmienia się czas realizacji projektu, koszty, a także inne wyzwania jeżeli chodzi o aspekty związane z bezpieczeństwem rozwiązania. Znacząco różnie może też wyglądać późniejszy etap wdrożenia aplikacji i wymagania co do potrzebnych zasobów. 
\section{Poziom skomplikowania kodu a domeny}
Pierwszą rzeczą nad którą warto się skupić jest poziom skomplikowania kodu w obu podejściach. Zdecydowanie mniej kodu źródłowego, mniej zewnętrznych zależności jest przy implementowaniu aplikacji monolitycznej.\index{aplikacja monolityczna}

Aplikację w wydaniu Event Sourcing'owym\index{Event Sourcing} autor pracy pisał o wiele dłużej i wymagała stworzenie o wiele więcej kodu w pierwszej fazie implementacji. Wysnuwany został pewien empiryczny wniosek, że mimo, iż zwłaszcza na początku jest o wiele więcej kodu źródłowego do zaimplementowania, to potem bardzo logicznie i łatwo można system rozwijać o nowe funkcjonalności, co nie jest takie proste przy aplikacji monolitycznej w zaawansowanym stadium tworzenia systemu.\index{aplikacja monolityczna}

Aplikacje oparte na mikroserwisach w połączeniu z Event Sourcing'iem\index{Event Sourcing} są bardziej naturalnym wyborem przy bardzo skomplikowanej domenie biznesowej. Przy prostym sklepie internetowym gdzie domena jest bardzo prosta a encji jest niewiele, użycie Event Sourcing'u\index{Event Sourcing} jest niekoniecznie optymalnym wyborem, bo po pierwsze domena nie będzie się w żaden sposób rozwijała i nie jest domeną nową i nieznaną. Implementacja modelu bazodanowego sklepu internetowego jest bardzo prosta nawet do znalezienia w Internecie i wystarczy ją zmodyfikować i zaaplikować do własnych potrzeb.

Tym bardziej skomplikowana jest domena tym większa szansa, że powinniśmy ją podzielić na mikroserwisy i coraz bardziej optymalnym staje się również użycie Event Sourcing'u\index{Event Sourcing} jako sposobu na persystencję danych i komunikację między aplikacjami.

Inwestycja w Event Sourcing\index{Event Sourcing} i szkolenie w tej dziedzinie zespołu programistów zacznie się zwracać przy skomplikowanej domenie biznesowej, jeżeli chodzi o poziom skomplikowania kodu.

\subsection{Wymagania systemowe}
Oczywistością jest, iż Event Sourcing\index{Event Sourcing} będzie wymagał o wiele więcej zasobów sprzętowych ze względu na jego charakter mikroserwisowy i potrzebę działania brokera aplikacji Apache Kafka. Kafka zapisuje wszystkie dane w systemie plików i potrzebne jest o wiele więcej miejsca na dysku twardym niż w przypadku drugiego podejścia. Mamy o wiele więcej danych, bo wszystkie zmiany w systemie są zapisane i możliwe jest ustawienie strategii nieusuwania żadnych historycznych danych, przez co bardzo szybko rośnie ilość wymaganego miejsca na dysku twardym. Dodatkowo Kafka opiera się na bazie danych RocksDB zapisująca dane w pamięci, przez co wymagania co do pamięci RAM również wzrastają z napływem coraz więcej ilości danych. 
Podczas implementacji aplikacji autor pracy zauważył, że bardzo dużo danych zostało zduplikowanych wielokrotnie. Informacja, która została wysłana jako wiadomość na broker Kafki na temat orderComponentEvent zostaje potem przetransformowana i przeniesiona na inne tematy. 
Z taką sytuacją autor pracy spotyka się również w pracy zawodowej przy implementacji tego typu systemów, co uważa za naturalną konsekwencję użycia Kafka Streams API, gdzie zasadność używania strumieni jest wtedy kiedy strumień przekazuje wiadomość na kolejny strumień, znajdujący swój początek na innym temacie wiadomości.

Jeżeli mamy ograniczone zasoby sprzętowe to zdecydowanie bezpieczniejszym rozwiążaniem jest użycie podejścia monolitycznego, tym bardziej, że przy wdrożeniu produkcyjnym Kafkę zaleca się konfigurować w klastrze i o wielu wątkach, co zwiększa jeszcze bardziej wymogi sprzętowe. Trzeba jednak mieć na uwadze, żeby nie wpaść w pułapkę zastawioną przez Anemic Model\index{Anemic Model}.

\subsection{Wymagania biznesowe}
Event Sourcing\index{Event Sourcing} ze swojej natury implikuje zapis większej ilości danych, co w niektórych branżach i ich specyficznych domenach biznesowych jest wymagane.  Posiadanie historii wszystkich zdarzeń w systemie pozwala na wysyłanie ich do hurtowni danych i tworzenie na ich podstawie statystyk i prognoz potrzebnych w funkcjonowaniu danego biznesu. 
Atawistyczna możliwość zapisywania wszystkich wiadomości w systemie pociąga za sobą również możliwości podniesienia kontroli nad systemem jak i jego bezpieczeństwa. Można wykryć nieprawidłowości w prosty sposób. Istotne też jest, że mamy szansę na odtworzenie całego systemu lub cofnięcie się do danego momentu w czasie, co np. może być bardzo przydatne w świecie bankowości.
Przy braku Event Sourcing'u\index{Event Sourcing} jakakolwiek aktualizacja czy usunięcie danych zmienia stan aplikacji w sposób trwały, tym samym tracąc informację o wcześniejszego stanu sprzed aktualizacji czy usunięcia.

\subsection{Wiedza programistyczna}
Bardzo dużym problemem jest zmiana sposobu myślenia dla programisty przy tworzeniu aplikacji opartych o wydarzenia i Event Sourcing.\index{Event Sourcing} Jest to zupełnie nowatorski sposób pisania aplikacji, który dla wielu, mimo wieloletniego doświadczenia w programowaniu aplikacji, jest czymś zupełnie nowym. Dlatego też potrzebna jest inwestycja dodatkowego czasu i wysiłku na wyszkolenie zespołu programistów w tej kwestii. 
Jeżeli projekt jest krótki i żaden z programistów w zespole nie miał styczności ani z Event Sourcingiem\index{Event Sourcing} ani z Apache Kafką, osobiście nie polecałbym przejścia na ten typ architektury, jako, że technologia ta ma dosyć wysoki próg wejścia i lepiej mieć w zespole mentora z tej dziedziny.

\section{Jakość oprogramowania}
Decyzje związane z wyborem podejścia i architektury systemu rzutuje na jakość oprogramowania, czyli tak naprawdę ostateczne powodzenie projektu lub porażkę.

Według międzynarodowego standardu ISO/IEC 25010:2011 jakość oprogramowania
to stabilność funkcjonalna, wydajność działania, kompatybilność, użyteczność, niezawodność, bezpieczeństwo, utrzymywalność i przenośność wytworzonego oprogramowania.\cite{jakoscOpr}

W świecie projektowania architektury aplikacji prężnie rozwijają się nowe podejścia, które powstały na przestrzeni ostatnich kilku lat. Czy Event Sourcing\index{Event Sourcing} jako jeden z nich ma wysoką jakość oprogramowania? 
\subsection{Funkcjonalna stabilność}
Posiadanie większej ilości mikroserisów ma dwie strony medalu. Trzeba o wiele więcej wysiłku w zarządzaniu każdą z tych aplikacji, tym bardziej w środowisku produkcyjnym, gdzie jest działających wiele instancji jednego mikroserwisu. Jednakże, aplikacja monolityczna,\index{aplikacja monolityczna} jeżeli zostanie wyłączona, to system zupełnie przestaje odpowiadać, przez co wszystkie operacje w systemie jednocześnie zostaną zablokowane, w tym procesowanie aktualnych żądań. Takiej sytuacji nie ma przy zastosowaniu mikroserisów, bowiem zawsze chociaż część komponentów będzie w stanie przetwarzać żądania.

Jednakże dopiero zastosowanie Event Sourcing'u\index{Event Sourcing} w połączeniu z mikroserwisami daje nam jak największą funkcjonalną stabilność, bowiem jeżeli któryś z komponentów nie będzie działał, to i tak broker Kafka będzie posiadał wszystkie wiadomości, które mają być przetworzone przez dany komponent i jak awaria zostania naprawiona, żadna informacja nie zostanie pominięta, a wszystkie zaległe wiadomości zostaną odebrane i przetworzone przez dany mikroserwis.

\subsubsection{Niebezpieczeństwa związane z wydarzeniami}
Funkcjonalna stabilność w asynchronicznych rozwiązaniach opartych o wydarzenia może zostać zagrożona, jeżeli nie będziemy świadomi pewnych niebezpieczństw, które implikuje natura takich systemów. Autor tekstu zauważył podczas testów empirycznych, że może wystąpić sytuacja wiadomości-duplikatów w sytuacji jak użytkownik kliknie kontrolkę w interfejsie wielokrotnie. Taka sytuacja musi być obsłużona i zaplanowana. Kolejną sytuacją jest to, że dany mikroserwis może otrzymać parę wiadomości na raz ze względu na opóźnienie lub brak dostępności mikroserwisu. Taki scenariusz może spowodować, że wydarzenia będą nieprzetworzone w kolejności w jakiej zostały dostarczone do Kafki, a na to nie mamy wpływu lub niektóre wiadomości będa operować na danych zdezaktualizowanych. Aplikacja orderComponent została zabezpieczona od takiego scenariusza. Niewykrycie takich sytuacji w testach i brak implementacji ze względu na przez brak doświadczenia z tego typu systemami może jednoznacznie pogorszyć jakość oprogramowania.

\subsection{Niezawodność}
Oba podejścia są jednakowo niezawodne, ponieważ to wszystko zależy od późniejszej konfiguracji systemu. Aplikacje oparte na Kafce jednakże wymagają o wiele czasu i specjalistycznej wiedzy, żeby system uczynić niezawodnym. Jeżeli projekt nie posiada w zespole specjalisty od wdrażania Kafki produkcyjnie, na pewno bardziej niezawodnym będzie stworzenie aplikacji monolitycznej,\index{aplikacja monolityczna} której zasady wdrażania są bliższe o wiele większej liczbie osób.

\subsection{Wydajność}
Autor pracy skupił się na aspektach implementacyjnych obu aplikacji, dlatego nie przedstawia dokładnych danych porównujących wydajność zapytań dla tej samej operacji w obu systemach. Byłoby to porównanie nieobiektywne, bowiem oba systemy mają odrebną naturę. W monolicie\index{aplikacja monolityczna} wszystkie żądania są przetwarzane synchronicznie a w Event Sourcingu\index{Event Sourcing} opieramy się na eventach, które z natury są asynchroniczne. Nawet bez posiadania mocnego sprzętu Kafka jest w stanie obsługiwać dane o dużej objętości. Ponadto, jest w stanie obsługiwać przepustowość tysięcy wiadomości na sekundę. Dlatego sam broker nie jest punktem, który pogarszałby wydajność. Raczej sama implementacja mikroserwisów i przepustowość łącza może wpłynąć na ogólną wydajność systemu.

\subsection{Użyteczność}
Użyteczność dla użytkownika końcowego w obu podejściach jest taka sama, ponieważ autor pracy dołożył wszelkich starań, żeby funkcjonalność obu implementacji systemu była bardzo zbliżona do siebie. 
Dla programisty jest o wiele większy próg wejścia do wersji systemu opartego na Event Sourcing'u,\index{Event Sourcing} ze względu na potrzebę poznania i zrozumienia zupełnie odmiennego podejścia niż w przypadku aplikacji monolitycznej.\index{aplikacja monolityczna}

\subsection{Bezpieczeństwo}
Bezpieczeństwo systemu jest o wiele bardziej zagrożone w przypadku architektury mikroserwisowej. Każda z aplikacji powinna mieć szyfrowanie SSL podczas wysyłki lub odbiorze wiadomości między sobą, a także enkrypcja i dekrypcja wiadomości na tematach Kafki. Również pojawia się temat autoryzacji i autentykacji danych mikroserwisów do danych tematów. Kafka natywnie wspiera różne poziomy bezpieczeństwa, ale faktem jest, że większa ilość mikroserwisów stwarza większe niebezpieczeństwo, któego czasami nie jesteśmy w stanie przewidzieć podczas implementacji systemu.

Z kolei bezpieczeństwo danych jest lepiej zapewniane przez Event Sourcing\index{Event Sourcing} ze względu na fakt, że posiadamy więcej danych na których podstawie możemy odtworzyć cały system przy pomyłkowym zastąpieniu istniejących danych lub cofnąć się w czasie do danego momentu w przypadku nieautoryzowanej operacji w systemie lub innego ataku osób trzecich. Możemy również te dane rozumieć z perspektywy ich zmian, tworząc za pomocą Kafka Streams API interaktywne zapytania.

\subsection{Kompatybilność}
Oba podejścia są podobnie kompatybilne i wszystkie elementy danego systemu ściśle i w naturalny sposób współdziałają ze sobą. 
Auto pracy stwierdza, iż w obu podejściach dosyć dużym problemem jest zmiana modelu encji np. przez wprowadzenie lub usunięcie jakiegoś pola encji (mowię tu o wersjonowaniu encji). W aplikacji monolitycznej\index{aplikacja monolityczna} nie jest to proste do zrealizowania w sytutacji, kiedy mamy już w danej tabeli bazy danych wiele wierszy. Tak samo nie jest sytuacja ułatwiona w przypadku zmiany modelu wydarzenia w systemie opartego na Event Sourcing'u, gdzie Kafka od razu zacznie wysyłać błędy o problemach z serializacją i deserializacją danych.\index{Event Sourcing}

\subsection{Utrzymywalność}
Utrzymywalność aplikacji monolitycznych\index{aplikacja monolityczna} jest na pewno mniej skomplikowana dla zespołu wsparcia. Po wdrożeniu aplikacji opartej na Event Sourcing'u, błędy jakie mogą wystąpić w danych są o wiele trudniejsze do wykrycia, bo nie jesteśmy w stanie prosto wyciągnąć informacji z serwera Kafki, tak jak w przypadku zapytań SQL w systemie opartym o bazy relacyjne. Również kwestia ręcznej podmiany danych, jaka umożliwiona jest przez komendy SQL nie jest aplikowalna do świata opartego na wydarzeniach Kafki.

Nie ma również dobrze rozwiniętych na tę chwilę narzędzi do monitorowania systemu opartego na Kafce, których to dla aplikacji monolitycznych\index{aplikacja monolityczna} jest dostępnych bardzo wiele. 

Musimy jednak pamiętać, że dalszy rozwój aplikacji i wgrywanie poprawek jest o wiele bardziej skomplikowane przy monolicie.\index{aplikacja monolityczna}  Kod staje się coraz bardziej niejasny i zagnieżdżony. Modularność, jaką dostarcza podzielenie systemu na mikroserwisy, pomaga we wgrywaniu poprawek bezpośrednio do danego mikroserwisu, co czyni tę procedurę łatwą i zwinną, bo jedynie ta konkretna część systemu jest dotknięta zmianami a nie całe środowisko. Dodatkowym skomplikowaniem w utrzymaniu aplikacji, która opiera się o model anemiczny,\index{Anemic Model} jest jeszcze większa możliwość naruszenia innej części aplikacji, zupełnie niezwiązanej ze zmianą którą dokonujemy w kodzie. Dzieje się to ze względu na ścisłą relację między kodem a bazą danych, gdzie często nazwy metod nie są wystarczająco bliskie nazewnictwu funkcjonalności domeny biznesowej.

Testowalność rozwiązań opartych o asynchroniczne wydarzenia, z obserwacji autora pracy, jest o wiele trudniejsze, wymaga napisania o wiele większej ilości dodatkowych komponentów do testowania, opartych o protokół dostarczony przez Kafkę.

\subsection{Przenaszalność}
Kafka została zaprojektowana w taki sposób, że może być skalowalna bez przestojów poprzez dodawanie nowych węzłów serwera w klastrze. Ponadto opiera się na idei replikacji i partycjonowania, co pozwala rozszerzać instalację Kafki o kolejne serwery bez potrzeby ponownego zupełnego restartowania całego brokera. Kafka Streams API wspiera wielowątkowość, co pomaga w skalowalności systemu w przypadku dodawania nowych partycji Kafki. Ta funckjonalność wpisuje się doskonale w naturę mikroserwisów, gdzie skalowalność systemu jest o wiele bardziej ułatwiona. Każdy mikroserwis niezależnie można replikować lub zwiększać i zmniejszać ilość jego instancji. 

W aplikacji monolitycznej\index{aplikacja monolityczna} jesteśmy zdani tylko i wyłącznie na skalowanie ilości instancji, gdzie zawsze cała aplikacja ze wszystkimi jej modułami funkcjonalnymi (a nie tylko jej część jak w przypadku mikroserwisów) jest uruchamiana w klasterze.

Łatwość adaptacyjna mikroserwisów pozwala na ich zmianę i szybkie wprowadzenie zmian tylko w obrębie danego mikroserwisu, podczas gdy zrestartowanie aplikacji monolitycznej trwa o wiele dłużej.
